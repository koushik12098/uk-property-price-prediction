{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6bb41b5-f526-4a1c-88bd-6c57b48fd557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/26 08:09:27 WARN Utils: Your hostname, KoushikPC resolves to a loopback address: 127.0.1.2; using 10.255.255.254 instead (on interface lo)\n",
      "26/02/26 08:09:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/26 08:09:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/26 08:09:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/koushik/spark\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"7006SCN_Feature_Engineering\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11983c73-2d7e-460e-b059-fbfb7869e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 30856185\n",
      "Columns: 9\n",
      "+-----+-------------------+--------+---------+-------+--------+-------------+---------+---------------+\n",
      "|price|               date|postcode|prop_type|old_new|duration|         town| district|         county|\n",
      "+-----+-------------------+--------+---------+-------+--------+-------------+---------+---------------+\n",
      "|75000|1999-11-19 00:00:00| DY5 4PZ|        D|      N|       F|BRIERLEY HILL|   DUDLEY|  WEST MIDLANDS|\n",
      "|49995|1999-10-28 00:00:00| DN6 7UP|        T|      Y|       F|    DONCASTER|DONCASTER|SOUTH YORKSHIRE|\n",
      "|79995|1999-06-11 00:00:00| IG1 1YF|        T|      N|       F|       ILFORD|REDBRIDGE| GREATER LONDON|\n",
      "+-----+-------------------+--------+---------+-------+--------+-------------+---------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/koushik/pp_parquet\")\n",
    "\n",
    "print(\"Rows:\", df.count())\n",
    "print(\"Columns:\", len(df.columns))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69395ca4-ef8e-4a82-a7df-c858ad454149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- prop_type: string (nullable = true)\n",
      " |-- old_new: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- town: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/26 08:10:28 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:======================================================>  (19 + 1) / 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------+---------+--------+--------+--------------+---------+--------+\n",
      "|summary|            price|postcode|prop_type| old_new|duration|          town| district|  county|\n",
      "+-------+-----------------+--------+---------+--------+--------+--------------+---------+--------+\n",
      "|  count|         30856185|30856185| 30856185|30856185|30856185|      30856185| 30856185|30856185|\n",
      "|   mean|233159.2306302286|    NULL|     NULL|    NULL|    NULL|          NULL|     NULL|    NULL|\n",
      "| stddev|955949.1723705888|    NULL|     NULL|    NULL|    NULL|          NULL|     NULL|    NULL|\n",
      "|    min|                1| AL1 1AJ|        D|       N|       F|ABBOTS LANGLEY|ABERCONWY|    AVON|\n",
      "|    max|        900000000|YO91 1RT|        T|       Y|       U| YSTRAD MEURIG|     YORK|    YORK|\n",
      "+-------+-----------------+--------+---------+--------+--------+--------------+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bee7494-7f04-4503-b8a6-cab5f57b32ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|               date|year|\n",
      "+-------------------+----+\n",
      "|1999-11-19 00:00:00|1999|\n",
      "|1999-10-28 00:00:00|1999|\n",
      "|1999-06-11 00:00:00|1999|\n",
      "|1999-03-01 00:00:00|1999|\n",
      "|1999-12-09 00:00:00|1999|\n",
      "+-------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df = df.withColumn(\"year\", year(\"date\"))\n",
    "\n",
    "df.select(\"date\", \"year\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad1d063-7a47-4221-962f-c87046c4ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922a7150-9e6d-4d18-a710-deb45d88ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prop_type : 5\n",
      "old_new : 2\n",
      "duration : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "district : 467\n",
      "county : 132\n"
     ]
    }
   ],
   "source": [
    "for c in [\"prop_type\", \"old_new\", \"duration\", \"district\", \"county\"]:\n",
    "    print(c, \":\", df.select(c).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36cd94ee-c9e8-42ea-9e65-ea41f44ce65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p\n",
    "\n",
    "df = df.withColumn(\"log_price\", log1p(\"price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13f1af1-78cf-4794-b4cd-4911112073e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------------+\n",
      "|features                                             |log_price         |\n",
      "+-----------------------------------------------------+------------------+\n",
      "|(610,[2,5,7,40,480,609],[1.0,1.0,1.0,1.0,1.0,1999.0])|11.225256725762893|\n",
      "|(610,[0,6,7,33,488,609],[1.0,1.0,1.0,1.0,1.0,1999.0])|10.819698281210112|\n",
      "|(610,[0,5,7,62,477,609],[1.0,1.0,1.0,1.0,1.0,1999.0])|11.289731912405976|\n",
      "+-----------------------------------------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "categorical_cols = [\"prop_type\", \"old_new\", \"duration\", \"district\", \"county\"]\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid=\"keep\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_vec\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[c+\"_vec\" for c in categorical_cols] + [\"year\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "df_features = pipeline.fit(df).transform(df)\n",
    "\n",
    "df_features.select(\"features\", \"log_price\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9527f9ca-d446-4f6f-9194-0d7fffcb29b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Rows: 30856185\n",
      "+--------------------+------------------+\n",
      "|            features|         log_price|\n",
      "+--------------------+------------------+\n",
      "|(610,[2,5,7,40,48...|11.225256725762893|\n",
      "|(610,[0,6,7,33,48...|10.819698281210112|\n",
      "|(610,[0,5,7,62,47...|11.289731912405976|\n",
      "+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model = df_features.select(\"features\", \"log_price\")\n",
    "\n",
    "print(\"Final Dataset Rows:\", df_model.count())\n",
    "df_model.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3ac214-1091-4933-b6a5-cb00293eb79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_model.write.mode(\"overwrite\").parquet(\"/home/koushik/pp_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006d3bd-d17b-454b-ba99-7a1966cf2bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
