{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae22f8ff-d772-417f-a4ef-32852aef86cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/26 11:46:04 WARN Utils: Your hostname, KoushikPC resolves to a loopback address: 127.0.1.2; using 10.255.255.254 instead (on interface lo)\n",
      "26/02/26 11:46:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/26 11:46:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/26 11:46:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/koushik/spark\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"4_Distributed_Processing\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1554a1ca-0f1a-441a-a202-8349c07bd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTRIBUTED PROCESSING \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Persisting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 30,856,185 | Persist time: 54.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "\n",
    "print(\"DISTRIBUTED PROCESSING \")\n",
    "\n",
    "df = spark.read.parquet(\"/home/koushik/pp_features\")\n",
    "\n",
    "print(\"\\n[1] Persisting data...\")\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "t = time.time()\n",
    "total = df.count()\n",
    "print(f\"    Rows: {total:,} | Persist time: {time.time()-t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b96abcc-9449-4896-b43a-cfb50fb94e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Broadcast Join...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Joined rows  : 30,856,185\n",
      "    Join time    : 2.52s\n",
      "    Sample:\n",
      "+---------+----------------+------+\n",
      "|prop_type|prop_description| price|\n",
      "+---------+----------------+------+\n",
      "|        D|        Detached| 75000|\n",
      "|        T|        Terraced| 49995|\n",
      "|        T|        Terraced| 79995|\n",
      "|        S|   Semi-Detached|151000|\n",
      "|        S|   Semi-Detached|146500|\n",
      "+---------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n Broadcast Join...\")\n",
    "\n",
    "df_raw = spark.read.parquet(\"/home/koushik/pp_parquet\")\n",
    "\n",
    "\n",
    "prop_lookup = spark.createDataFrame([\n",
    "    (\"D\", \"Detached\"),\n",
    "    (\"S\", \"Semi-Detached\"),\n",
    "    (\"T\", \"Terraced\"),\n",
    "    (\"F\", \"Flat/Maisonette\"),\n",
    "    (\"O\", \"Other\")\n",
    "], [\"prop_type\", \"prop_description\"])\n",
    "\n",
    "t = time.time()\n",
    "df_joined = df_raw.join(\n",
    "    broadcast(prop_lookup),\n",
    "    on=\"prop_type\",\n",
    "    how=\"left\"\n",
    ")\n",
    "count = df_joined.count()\n",
    "join_time = time.time() - t\n",
    "\n",
    "print(f\"    Joined rows  : {count:,}\")\n",
    "print(f\"    Join time    : {join_time:.2f}s\")\n",
    "print(\"    Sample:\")\n",
    "df_joined.select(\"prop_type\", \"prop_description\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e3b5a9-b4a3-4709-9edf-ff967c40ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Error Handling with Data Lineage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Loaded: /home/koushik/pp_features → 30,856,185 rows\n",
      "     Failed to load /home/koushik/nonexistent_path: [PATH_NOT_FOUND] Path does not exist: file:/home/koushik/nonexistent_path.\n",
      "\n",
      "[4] Unpersisting to free memory...\n",
      "     df unpersisted\n",
      "\n",
      "[5] Persist only model-ready data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============>                                           (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     train_df persisted: 4,939,684 rows\n",
      "     test_df  persisted: 1,235,407 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n Error Handling with Data Lineage...\")\n",
    "\n",
    "def safe_load_parquet(path):\n",
    "    try:\n",
    "        df = spark.read.parquet(path)\n",
    "        count = df.count()\n",
    "        print(f\"     Loaded: {path} → {count:,} rows\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"     Failed to load {path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def safe_model_train(model, train_df, name=\"Model\"):\n",
    "    try:\n",
    "        t = time.time()\n",
    "        fitted = model.fit(train_df)\n",
    "        print(f\"     {name} trained in {time.time()-t:.1f}s\")\n",
    "        return fitted\n",
    "    except MemoryError:\n",
    "        print(f\"     {name} failed: Out of Memory — try smaller sample\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"     {name} failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "df_features = safe_load_parquet(\"/home/koushik/pp_features\")\n",
    "df_missing   = safe_load_parquet(\"/home/koushik/nonexistent_path\")  # will fail safely\n",
    "\n",
    "print(\"\\n[4] Unpersisting to free memory...\")\n",
    "df.unpersist()\n",
    "print(\"     df unpersisted\")\n",
    "\n",
    "print(\"\\n[5] Persist only model-ready data...\")\n",
    "df_sample = df_features.sample(fraction=0.2, seed=42)\n",
    "train_df, test_df = df_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count  = test_df.count()\n",
    "print(f\"     train_df persisted: {train_count:,} rows\")\n",
    "print(f\"     test_df  persisted: {test_count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "372816a8-52bc-46ed-9cc1-bd2decc234c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Random Forest trained in 121.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Decision Tree trained in 41.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest → RMSE: 0.5809 | R2: 0.5393\n",
      "\n",
      "Decision Tree → RMSE: 0.5789 | R2: 0.5424\n",
      "\n",
      " All caches cleared — memory freed\n",
      " Distributed Processing Complete\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"log_price\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rf  = safe_model_train(\n",
    "    RandomForestRegressor(featuresCol=\"features\", labelCol=\"log_price\",\n",
    "                          numTrees=20, maxDepth=6, maxBins=64, seed=42),\n",
    "    train_df, \"Random Forest\"\n",
    ")\n",
    "\n",
    "dt  = safe_model_train(\n",
    "    DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"log_price\",\n",
    "                          maxDepth=6, maxBins=64, seed=42),\n",
    "    train_df, \"Decision Tree\"\n",
    ")\n",
    "\n",
    "for name, model in [(\"Random Forest\", rf), (\"Decision Tree\", dt)]:\n",
    "    if model is not None:\n",
    "        pred = model.transform(test_df)\n",
    "        rmse = evaluator.setMetricName(\"rmse\").evaluate(pred)\n",
    "        r2   = evaluator.setMetricName(\"r2\").evaluate(pred)\n",
    "        print(f\"\\n{name} → RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "print(\"\\n All caches cleared — memory freed\")\n",
    "print(\" Distributed Processing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563a4dd6-86fa-4c68-8385-60c66c1f6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame avg log_price : 11.9393\n",
      "DataFrame Time          : 0.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD avg log_price       : 11.9393\n",
      "RDD Time                : 33.93s\n",
      "\n",
      "DataFrame is 136.49x FASTER than RDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "df_result = df.groupBy().avg(\"log_price\").collect()\n",
    "df_time = time.time() - t\n",
    "print(f\"DataFrame avg log_price : {df_result[0][0]:.4f}\")\n",
    "print(f\"DataFrame Time          : {df_time:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "rdd = df.select(\"log_price\").rdd.map(lambda x: x[0])\n",
    "rdd_count  = rdd.count()\n",
    "rdd_sum    = rdd.sum()\n",
    "rdd_avg    = rdd_sum / rdd_count\n",
    "rdd_time   = time.time() - t\n",
    "print(f\"\\nRDD avg log_price       : {rdd_avg:.4f}\")\n",
    "print(f\"RDD Time                : {rdd_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nDataFrame is {rdd_time/df_time:.2f}x FASTER than RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49664991-ff3a-4969-a735-630f92e073e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "print(\"===== STRONG SCALING =====\")\n",
    "print(\"Same data size, increasing cores\\n\")\n",
    "\n",
    "DATA_PATH = \"/home/koushik/pp_features\"\n",
    "FRACTION  = 0.1   # fixed data size\n",
    "\n",
    "cores_list    = [1, 2, 4]\n",
    "strong_times  = []\n",
    "strong_rmse   = []\n",
    "\n",
    "for cores in cores_list:\n",
    "    # Restart spark with different cores\n",
    "    spark.stop()\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"Strong_Scaling_{cores}cores\") \\\n",
    "        .master(f\"local[{cores}]\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    df = spark.read.parquet(DATA_PATH).sample(fraction=FRACTION, seed=42)\n",
    "    tr, te = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    tr.cache(); tr.count()\n",
    "\n",
    "    t = time.time()\n",
    "    model = DecisionTreeRegressor(\n",
    "        featuresCol=\"features\", labelCol=\"log_price\",\n",
    "        maxDepth=5, maxBins=32\n",
    "    ).fit(tr)\n",
    "    elapsed = time.time() - t\n",
    "\n",
    "    strong_times.append(elapsed)\n",
    "    rows = tr.count()\n",
    "    print(f\"Cores: {cores} | Rows: {rows:,} | Time: {elapsed:.1f}s\")\n",
    "    tr.unpersist()\n",
    "\n",
    "speedup = [strong_times[0]/t for t in strong_times]\n",
    "efficiency = [s/c * 100 for s, c in zip(speedup, cores_list)]\n",
    "\n",
    "print(\"\\n--- Strong Scaling Summary ---\")\n",
    "for c, t, s, e in zip(cores_list, strong_times, speedup, efficiency):\n",
    "    print(f\"Cores: {c} | Time: {t:.1f}s | Speedup: {s:.2f}x | Efficiency: {e:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67625bd7-a974-43ea-902e-4a961a6ae9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRONG SCALING\n",
      "Same data size, increasing cores\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 1 | Rows: 2,469,749 | Time: 52.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 2 | Rows: 2,469,749 | Time: 28.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============>                                           (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 4 | Rows: 2,469,260 | Time: 16.1s\n",
      "\n",
      "--- Strong Scaling Summary ---\n",
      "Cores: 1 | Time: 52.1s | Speedup: 1.00x | Efficiency: 100.0%\n",
      "Cores: 2 | Time: 28.2s | Speedup: 1.85x | Efficiency: 92.3%\n",
      "Cores: 4 | Time: 16.1s | Speedup: 3.25x | Efficiency: 81.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "print(\"STRONG SCALING\")\n",
    "print(\"Same data size, increasing cores\\n\")\n",
    "\n",
    "DATA_PATH = \"/home/koushik/pp_features\"\n",
    "FRACTION  = 0.1 \n",
    "cores_list    = [1, 2, 4]\n",
    "strong_times  = []\n",
    "strong_rmse   = []\n",
    "\n",
    "for cores in cores_list:\n",
    "    # Restart spark with different cores\n",
    "    spark.stop()\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"Strong_Scaling_{cores}cores\") \\\n",
    "        .master(f\"local[{cores}]\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    df = spark.read.parquet(DATA_PATH).sample(fraction=FRACTION, seed=42)\n",
    "    tr, te = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    tr.cache(); tr.count()\n",
    "\n",
    "    t = time.time()\n",
    "    model = DecisionTreeRegressor(\n",
    "        featuresCol=\"features\", labelCol=\"log_price\",\n",
    "        maxDepth=5, maxBins=32\n",
    "    ).fit(tr)\n",
    "    elapsed = time.time() - t\n",
    "\n",
    "    strong_times.append(elapsed)\n",
    "    rows = tr.count()\n",
    "    print(f\"Cores: {cores} | Rows: {rows:,} | Time: {elapsed:.1f}s\")\n",
    "    tr.unpersist()\n",
    "\n",
    "speedup = [strong_times[0]/t for t in strong_times]\n",
    "efficiency = [s/c * 100 for s, c in zip(speedup, cores_list)]\n",
    "\n",
    "print(\"\\n--- Strong Scaling Summary ---\")\n",
    "for c, t, s, e in zip(cores_list, strong_times, speedup, efficiency):\n",
    "    print(f\"Cores: {c} | Time: {t:.1f}s | Speedup: {s:.2f}x | Efficiency: {e:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b395ab-f242-4f39-8ceb-41c87818fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== BOTTLENECK ANALYSIS =====\n",
      "\n",
      "[1] I/O Time (read+count)      : 0.21s\n",
      "[2] Shuffle Time (groupBy)     : 0.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Training (no cache)        : 27.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Training (with cache)      : 17.12s\n",
      "    Cache Speedup              : 1.59x\n",
      "\n",
      " Main Bottleneck: Compute (27.19s)\n",
      "Recommendation: Use persist() + increase driver memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"===== BOTTLENECK ANALYSIS =====\\n\")\n",
    "\n",
    "df = spark.read.parquet(DATA_PATH).sample(fraction=0.1, seed=42)\n",
    "\n",
    "# 1. I/O Bottleneck\n",
    "t = time.time()\n",
    "spark.read.parquet(DATA_PATH).sample(fraction=0.1, seed=42).count()\n",
    "io_time = time.time() - t\n",
    "print(f\"[1] I/O Time (read+count)      : {io_time:.2f}s\")\n",
    "\n",
    "# 2. Shuffle Bottleneck\n",
    "t = time.time()\n",
    "from pyspark.sql import functions as F\n",
    "df.groupBy(\"log_price\").count().collect()\n",
    "shuffle_time = time.time() - t\n",
    "print(f\"[2] Shuffle Time (groupBy)     : {shuffle_time:.2f}s\")\n",
    "\n",
    "# 3. Memory Bottleneck (no cache)\n",
    "tr, te = df.randomSplit([0.8, 0.2], seed=42)\n",
    "t = time.time()\n",
    "DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"log_price\",\n",
    "                       maxDepth=4, maxBins=32).fit(tr)\n",
    "no_cache_time = time.time() - t\n",
    "print(f\"[3] Training (no cache)        : {no_cache_time:.2f}s\")\n",
    "\n",
    "# 4. With Cache\n",
    "from pyspark import StorageLevel\n",
    "tr.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "tr.count()\n",
    "t = time.time()\n",
    "DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"log_price\",\n",
    "                       maxDepth=4, maxBins=32).fit(tr)\n",
    "cache_time = time.time() - t\n",
    "print(f\"[4] Training (with cache)      : {cache_time:.2f}s\")\n",
    "print(f\"    Cache Speedup              : {no_cache_time/cache_time:.2f}x\")\n",
    "tr.unpersist()\n",
    "\n",
    "# Identify main bottleneck\n",
    "times_dict = {\n",
    "    \"I/O\":     io_time,\n",
    "    \"Shuffle\": shuffle_time,\n",
    "    \"Compute\": no_cache_time\n",
    "}\n",
    "bottleneck = max(times_dict, key=times_dict.get)\n",
    "print(f\"\\n Main Bottleneck: {bottleneck} ({times_dict[bottleneck]:.2f}s)\")\n",
    "print(f\"Recommendation: \", end=\"\")\n",
    "if bottleneck == \"I/O\":\n",
    "    print(\"Use Parquet format + partitioning strategy\")\n",
    "elif bottleneck == \"Shuffle\":\n",
    "    print(\"Reduce shuffle partitions + use broadcast joins\")\n",
    "else:\n",
    "    print(\"Use persist() + increase driver memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd789f3c-601b-4f9f-9145-962eaf2a48cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
