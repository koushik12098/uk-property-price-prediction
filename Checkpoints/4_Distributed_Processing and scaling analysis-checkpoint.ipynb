{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae22f8ff-d772-417f-a4ef-32852aef86cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark path set successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "\n",
    "sys.path.insert(0, \"/home/koushik/spark/python\")\n",
    "py4j = glob.glob(\"/home/koushik/spark/python/lib/py4j-*.zip\")\n",
    "if py4j:\n",
    "    sys.path.insert(0, py4j[0])\n",
    "\n",
    "print(\"PySpark path set successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05be29fd-20ec-488e-95f2-c2fcf81b7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/koushik/spark\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"4_Distributed_Processing\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1554a1ca-0f1a-441a-a202-8349c07bd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTRIBUTED PROCESSING \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Persisting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 30,856,185 | Persist time: 54.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "\n",
    "print(\"DISTRIBUTED PROCESSING \")\n",
    "\n",
    "df = spark.read.parquet(\"/home/koushik/pp_features\")\n",
    "\n",
    "print(\"\\n[1] Persisting data...\")\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "t = time.time()\n",
    "total = df.count()\n",
    "print(f\"    Rows: {total:,} | Persist time: {time.time()-t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b96abcc-9449-4896-b43a-cfb50fb94e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Broadcast Join...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Joined rows  : 30,856,185\n",
      "    Join time    : 2.52s\n",
      "    Sample:\n",
      "+---------+----------------+------+\n",
      "|prop_type|prop_description| price|\n",
      "+---------+----------------+------+\n",
      "|        D|        Detached| 75000|\n",
      "|        T|        Terraced| 49995|\n",
      "|        T|        Terraced| 79995|\n",
      "|        S|   Semi-Detached|151000|\n",
      "|        S|   Semi-Detached|146500|\n",
      "+---------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n Broadcast Join...\")\n",
    "\n",
    "df_raw = spark.read.parquet(\"/home/koushik/pp_parquet\")\n",
    "\n",
    "\n",
    "prop_lookup = spark.createDataFrame([\n",
    "    (\"D\", \"Detached\"),\n",
    "    (\"S\", \"Semi-Detached\"),\n",
    "    (\"T\", \"Terraced\"),\n",
    "    (\"F\", \"Flat/Maisonette\"),\n",
    "    (\"O\", \"Other\")\n",
    "], [\"prop_type\", \"prop_description\"])\n",
    "\n",
    "t = time.time()\n",
    "df_joined = df_raw.join(\n",
    "    broadcast(prop_lookup),\n",
    "    on=\"prop_type\",\n",
    "    how=\"left\"\n",
    ")\n",
    "count = df_joined.count()\n",
    "join_time = time.time() - t\n",
    "\n",
    "print(f\"    Joined rows  : {count:,}\")\n",
    "print(f\"    Join time    : {join_time:.2f}s\")\n",
    "print(\"    Sample:\")\n",
    "df_joined.select(\"prop_type\", \"prop_description\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e3b5a9-b4a3-4709-9edf-ff967c40ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Error Handling with Data Lineage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Loaded: /home/koushik/pp_features → 30,856,185 rows\n",
      "     Failed to load /home/koushik/nonexistent_path: [PATH_NOT_FOUND] Path does not exist: file:/home/koushik/nonexistent_path.\n",
      "\n",
      "[4] Unpersisting to free memory...\n",
      "     df unpersisted\n",
      "\n",
      "[5] Persist only model-ready data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============>                                           (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     train_df persisted: 4,939,684 rows\n",
      "     test_df  persisted: 1,235,407 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n Error Handling with Data Lineage...\")\n",
    "\n",
    "def safe_load_parquet(path):\n",
    "    try:\n",
    "        df = spark.read.parquet(path)\n",
    "        count = df.count()\n",
    "        print(f\"     Loaded: {path} → {count:,} rows\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"     Failed to load {path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def safe_model_train(model, train_df, name=\"Model\"):\n",
    "    try:\n",
    "        t = time.time()\n",
    "        fitted = model.fit(train_df)\n",
    "        print(f\"     {name} trained in {time.time()-t:.1f}s\")\n",
    "        return fitted\n",
    "    except MemoryError:\n",
    "        print(f\"     {name} failed: Out of Memory — try smaller sample\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"     {name} failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "df_features = safe_load_parquet(\"/home/koushik/pp_features\")\n",
    "df_missing   = safe_load_parquet(\"/home/koushik/nonexistent_path\")  # will fail safely\n",
    "\n",
    "print(\"\\n[4] Unpersisting to free memory...\")\n",
    "df.unpersist()\n",
    "print(\"     df unpersisted\")\n",
    "\n",
    "print(\"\\n[5] Persist only model-ready data...\")\n",
    "df_sample = df_features.sample(fraction=0.2, seed=42)\n",
    "train_df, test_df = df_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count  = test_df.count()\n",
    "print(f\"     train_df persisted: {train_count:,} rows\")\n",
    "print(f\"     test_df  persisted: {test_count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "372816a8-52bc-46ed-9cc1-bd2decc234c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Random Forest trained in 121.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Decision Tree trained in 41.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest → RMSE: 0.5809 | R2: 0.5393\n",
      "\n",
      "Decision Tree → RMSE: 0.5789 | R2: 0.5424\n",
      "\n",
      " All caches cleared — memory freed\n",
      " Distributed Processing Complete\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"log_price\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rf  = safe_model_train(\n",
    "    RandomForestRegressor(featuresCol=\"features\", labelCol=\"log_price\",\n",
    "                          numTrees=20, maxDepth=6, maxBins=64, seed=42),\n",
    "    train_df, \"Random Forest\"\n",
    ")\n",
    "\n",
    "dt  = safe_model_train(\n",
    "    DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"log_price\",\n",
    "                          maxDepth=6, maxBins=64, seed=42),\n",
    "    train_df, \"Decision Tree\"\n",
    ")\n",
    "\n",
    "for name, model in [(\"Random Forest\", rf), (\"Decision Tree\", dt)]:\n",
    "    if model is not None:\n",
    "        pred = model.transform(test_df)\n",
    "        rmse = evaluator.setMetricName(\"rmse\").evaluate(pred)\n",
    "        r2   = evaluator.setMetricName(\"r2\").evaluate(pred)\n",
    "        print(f\"\\n{name} → RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "print(\"\\n All caches cleared — memory freed\")\n",
    "print(\" Distributed Processing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563a4dd6-86fa-4c68-8385-60c66c1f6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame avg log_price : 11.9393\n",
      "DataFrame Time          : 0.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD avg log_price       : 11.9393\n",
      "RDD Time                : 33.93s\n",
      "\n",
      "DataFrame is 136.49x FASTER than RDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "df_result = df.groupBy().avg(\"log_price\").collect()\n",
    "df_time = time.time() - t\n",
    "print(f\"DataFrame avg log_price : {df_result[0][0]:.4f}\")\n",
    "print(f\"DataFrame Time          : {df_time:.2f}s\")\n",
    "\n",
    "t = time.time()\n",
    "rdd = df.select(\"log_price\").rdd.map(lambda x: x[0])\n",
    "rdd_count  = rdd.count()\n",
    "rdd_sum    = rdd.sum()\n",
    "rdd_avg    = rdd_sum / rdd_count\n",
    "rdd_time   = time.time() - t\n",
    "print(f\"\\nRDD avg log_price       : {rdd_avg:.4f}\")\n",
    "print(f\"RDD Time                : {rdd_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nDataFrame is {rdd_time/df_time:.2f}x FASTER than RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49664991-ff3a-4969-a735-630f92e073e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== WEAK SCALING =====\n",
      "Data size increases proportionally with cores.\n",
      "Ideal outcome: time stays constant as both data & cores grow.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 1 | Rows: 395,468 | Time: 5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@19676aaf rejected from java.util.concurrent.ThreadPoolExecutor@17a097f3[Shutting down, pool size = 43, active threads = 3, queued tasks = 0, completed tasks = 45]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@331c9272 rejected from java.util.concurrent.ThreadPoolExecutor@17a097f3[Shutting down, pool size = 42, active threads = 3, queued tasks = 0, completed tasks = 45]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@569d9dc8 rejected from java.util.concurrent.ThreadPoolExecutor@17a097f3[Shutting down, pool size = 31, active threads = 3, queued tasks = 0, completed tasks = 45]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@39e4f902 rejected from java.util.concurrent.ThreadPoolExecutor@17a097f3[Shutting down, pool size = 16, active threads = 3, queued tasks = 0, completed tasks = 45]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@45968de0 rejected from java.util.concurrent.ThreadPoolExecutor@17a097f3[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 47]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@4f6ee690 rejected from java.util.concurrent.ThreadPoolExecutor@17a097f3[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 47]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 2 | Rows: 790,103 | Time: 6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@4506d8fc rejected from java.util.concurrent.ThreadPoolExecutor@4bc12e2c[Shutting down, pool size = 92, active threads = 1, queued tasks = 0, completed tasks = 99]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@afc2417 rejected from java.util.concurrent.ThreadPoolExecutor@4bc12e2c[Shutting down, pool size = 86, active threads = 1, queued tasks = 0, completed tasks = 99]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 4 | Rows: 1,604,816 | Time: 14.6s\n",
      "\n",
      "--- Weak Scaling Summary ---\n",
      " Cores |       Rows |  Time(s) |  Norm.Time | Efficiency\n",
      "---------------------------------------------------------\n",
      "     1 |    395,468 |      5.5 |      1.000 |     100.0%\n",
      "     2 |    790,103 |      6.7 |      1.206 |      82.9%\n",
      "     4 |  1,604,816 |     14.6 |      2.636 |      37.9%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/koushik/spark\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "print(\"===== WEAK SCALING =====\")\n",
    "print(\"Data size increases proportionally with cores.\")\n",
    "print(\"Ideal outcome: time stays constant as both data & cores grow.\\n\")\n",
    "\n",
    "DATA_PATH = \"/home/koushik/pp_features\"\n",
    "\n",
    "weak_configs = [\n",
    "    (1, 0.016),\n",
    "    (2, 0.032),\n",
    "    (4, 0.065),\n",
    "]\n",
    "\n",
    "weak_times = []\n",
    "weak_rows  = []\n",
    "\n",
    "for cores, fraction in weak_configs:\n",
    "    try:\n",
    "        spark.stop()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"Weak_Scaling_{cores}cores\") \\\n",
    "        .master(f\"local[{cores}]\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(cores * 10)) \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    df = spark.read.parquet(DATA_PATH).sample(fraction=fraction, seed=42)\n",
    "    tr, te = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    tr.cache()\n",
    "    actual_rows = tr.count()\n",
    "    weak_rows.append(actual_rows)\n",
    "\n",
    "    t = time.time()\n",
    "    DecisionTreeRegressor(\n",
    "        featuresCol=\"features\", labelCol=\"log_price\",\n",
    "        maxDepth=5, maxBins=32\n",
    "    ).fit(tr)\n",
    "    elapsed = time.time() - t\n",
    "\n",
    "    weak_times.append(elapsed)\n",
    "    print(f\"Cores: {cores} | Rows: {actual_rows:,} | Time: {elapsed:.1f}s\")\n",
    "    tr.unpersist()\n",
    "\n",
    "# Stop Spark BEFORE plotting\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Summary Table\n",
    "print(\"\\n--- Weak Scaling Summary ---\")\n",
    "print(f\"{'Cores':>6} | {'Rows':>10} | {'Time(s)':>8} | {'Norm.Time':>10} | {'Efficiency':>10}\")\n",
    "print(\"-\" * 57)\n",
    "for i, (cores, _) in enumerate(weak_configs):\n",
    "    norm = weak_times[i] / weak_times[0]\n",
    "    eff  = (1.0 / norm) * 100\n",
    "    print(f\"{cores:>6} | {weak_rows[i]:>10,} | {weak_times[i]:>8.1f} | {norm:>10.3f} | {eff:>9.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67625bd7-a974-43ea-902e-4a961a6ae9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRONG SCALING\n",
      "Same data size, increasing cores\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 1 | Rows: 2,469,749 | Time: 52.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 2 | Rows: 2,469,749 | Time: 28.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============>                                           (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 4 | Rows: 2,469,260 | Time: 16.1s\n",
      "\n",
      "--- Strong Scaling Summary ---\n",
      "Cores: 1 | Time: 52.1s | Speedup: 1.00x | Efficiency: 100.0%\n",
      "Cores: 2 | Time: 28.2s | Speedup: 1.85x | Efficiency: 92.3%\n",
      "Cores: 4 | Time: 16.1s | Speedup: 3.25x | Efficiency: 81.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "print(\"STRONG SCALING\")\n",
    "print(\"Same data size, increasing cores\\n\")\n",
    "\n",
    "DATA_PATH = \"/home/koushik/pp_features\"\n",
    "FRACTION  = 0.1 \n",
    "cores_list    = [1, 2, 4]\n",
    "strong_times  = []\n",
    "strong_rmse   = []\n",
    "\n",
    "for cores in cores_list:\n",
    "    # Restart spark with different cores\n",
    "    spark.stop()\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"Strong_Scaling_{cores}cores\") \\\n",
    "        .master(f\"local[{cores}]\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    df = spark.read.parquet(DATA_PATH).sample(fraction=FRACTION, seed=42)\n",
    "    tr, te = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    tr.cache(); tr.count()\n",
    "\n",
    "    t = time.time()\n",
    "    model = DecisionTreeRegressor(\n",
    "        featuresCol=\"features\", labelCol=\"log_price\",\n",
    "        maxDepth=5, maxBins=32\n",
    "    ).fit(tr)\n",
    "    elapsed = time.time() - t\n",
    "\n",
    "    strong_times.append(elapsed)\n",
    "    rows = tr.count()\n",
    "    print(f\"Cores: {cores} | Rows: {rows:,} | Time: {elapsed:.1f}s\")\n",
    "    tr.unpersist()\n",
    "\n",
    "speedup = [strong_times[0]/t for t in strong_times]\n",
    "efficiency = [s/c * 100 for s, c in zip(speedup, cores_list)]\n",
    "\n",
    "print(\"\\n--- Strong Scaling Summary ---\")\n",
    "for c, t, s, e in zip(cores_list, strong_times, speedup, efficiency):\n",
    "    print(f\"Cores: {c} | Time: {t:.1f}s | Speedup: {s:.2f}x | Efficiency: {e:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c272bb-4c6d-4232-ad33-81c01b77f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting numpy>=1.23 (from matplotlib)\n",
      "  Using cached numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from matplotlib) (26.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-12.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./myenv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "Using cached numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached pillow-12.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "Using cached pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Installing collected packages: pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 numpy-2.4.2 pillow-12.1.1 pyparsing-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b395ab-f242-4f39-8ceb-41c87818fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== BOTTLENECK ANALYSIS =====\n",
      "\n",
      "[1] I/O Time (read+count)      : 0.21s\n",
      "[2] Shuffle Time (groupBy)     : 0.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Training (no cache)        : 27.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Training (with cache)      : 17.12s\n",
      "    Cache Speedup              : 1.59x\n",
      "\n",
      " Main Bottleneck: Compute (27.19s)\n",
      "Recommendation: Use persist() + increase driver memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"===== BOTTLENECK ANALYSIS =====\\n\")\n",
    "\n",
    "df = spark.read.parquet(DATA_PATH).sample(fraction=0.1, seed=42)\n",
    "\n",
    "# 1. I/O Bottleneck\n",
    "t = time.time()\n",
    "spark.read.parquet(DATA_PATH).sample(fraction=0.1, seed=42).count()\n",
    "io_time = time.time() - t\n",
    "print(f\"[1] I/O Time (read+count)      : {io_time:.2f}s\")\n",
    "\n",
    "# 2. Shuffle Bottleneck\n",
    "t = time.time()\n",
    "from pyspark.sql import functions as F\n",
    "df.groupBy(\"log_price\").count().collect()\n",
    "shuffle_time = time.time() - t\n",
    "print(f\"[2] Shuffle Time (groupBy)     : {shuffle_time:.2f}s\")\n",
    "\n",
    "# 3. Memory Bottleneck (no cache)\n",
    "tr, te = df.randomSplit([0.8, 0.2], seed=42)\n",
    "t = time.time()\n",
    "DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"log_price\",\n",
    "                       maxDepth=4, maxBins=32).fit(tr)\n",
    "no_cache_time = time.time() - t\n",
    "print(f\"[3] Training (no cache)        : {no_cache_time:.2f}s\")\n",
    "\n",
    "# 4. With Cache\n",
    "from pyspark import StorageLevel\n",
    "tr.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "tr.count()\n",
    "t = time.time()\n",
    "DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"log_price\",\n",
    "                       maxDepth=4, maxBins=32).fit(tr)\n",
    "cache_time = time.time() - t\n",
    "print(f\"[4] Training (with cache)      : {cache_time:.2f}s\")\n",
    "print(f\"    Cache Speedup              : {no_cache_time/cache_time:.2f}x\")\n",
    "tr.unpersist()\n",
    "\n",
    "# Identify main bottleneck\n",
    "times_dict = {\n",
    "    \"I/O\":     io_time,\n",
    "    \"Shuffle\": shuffle_time,\n",
    "    \"Compute\": no_cache_time\n",
    "}\n",
    "bottleneck = max(times_dict, key=times_dict.get)\n",
    "print(f\"\\n Main Bottleneck: {bottleneck} ({times_dict[bottleneck]:.2f}s)\")\n",
    "print(f\"Recommendation: \", end=\"\")\n",
    "if bottleneck == \"I/O\":\n",
    "    print(\"Use Parquet format + partitioning strategy\")\n",
    "elif bottleneck == \"Shuffle\":\n",
    "    print(\"Reduce shuffle partitions + use broadcast joins\")\n",
    "else:\n",
    "    print(\"Use persist() + increase driver memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd789f3c-601b-4f9f-9145-962eaf2a48cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
